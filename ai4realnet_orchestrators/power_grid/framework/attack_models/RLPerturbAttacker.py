import pandas as pd
import numpy as np
import copy
from perturbation_agents.rl_perturb_agent import RLPerturbationAgent
from Grid2OpUtils import Grid2OpUtils
from attack_models.BaseAttackerClass import BaseAttackerClass
from utility.UtilityHelper import UtilityHelper

import traceback

class RLPerturbAttacker(BaseAttackerClass):
    """
    Reinforcement Learning (RL) Perturbation Attacker.
    This wrapper class perturbs environment observations using a RL based perturbation agent.
    It serves as an interface between the environment and the RL perturbation model.

    Args:
        model_path: Path to the saved RL perturbation model.
        target_path: Path to the target model.
        env: Environment instance to attack.
        agent: The victim agent
    
        
    Attributes:
        model_name: Identifier for the attacker model.
        pickle_file: Default filename for serialization.
        model_path: Path to the saved perturbation model
        target_path: Path to the target model
        env: Environment reference
        agent: Target under attack (victim agent)
        model: Perturbation model trained
    """
    def __init__(self, model_path, target_path, env, agent):
        try:
            print("[DEBUG] RL Agent initialized")
            self.model_name = "RLPerturbAttacker"
            self.pickle_file = "rl_perturb_attacker.pkl"
            self.model_path = model_path
            self.target_path = target_path
            self.env = env
            self.agent = agent

            # Load RL perturbation model 
            self.model = self.load_model()
            self.model.load_model(self.model_path, self.target_path)
            print("[DEBUG] loaded model inside the class")
        except Exception as e:
            print("[ERROR] RLPerturbAttacker init failed:", e)
            print(traceback.format_exc())
            raise



    def perturb(self, obs):
        """
        Apply RL based perturbation to an observation.

        Args:
            obs: Original environment observation.
        
        Returns:
            Perturbed observation generated by th RL model
        """
        obs_t = copy.deepcopy(obs) # Work on a copy to avoid mutating the original observation
        obs_perturbed = self.model.perturb(obs_t)
        return obs_perturbed

    def load_model(self):
        """
        Construct and configure the RL perturbation model.

        Returns:
            Configured perturbation agent ready for use.
        """



        # Attribute index mapping
        attr_list, attr_start_idx, _ = UtilityHelper.build_attr_list_and_index(self.env.get_obs())
        similarity_acts, similarity_acts_array, _, _ = UtilityHelper.init_similarity_score_dict(self.env, self.agent)
        # Identify groups of actions with similarity above 0.9
        similarity_threshold = 0.9
        subset_acts2 = UtilityHelper.select_representative_actions(similarity_acts_array, similarity_threshold)
        
        best_combinations = UtilityHelper.get_best_combinations(
            raw_data_path="attack_models/teacher_experience_triples_all.csv",
            reward_improv_threshold=0.1,
            std_factor=1.5
        )
        best_combinations = [int(x) for sublist in best_combinations for x in sublist]
        # DEBUG: Print what we got
        print(f"[DEBUG] best_combinations length: {len(best_combinations)}")
        print(f"[DEBUG] best_combinations (first 20): {best_combinations[:20]}")
        print(f"[DEBUG] subset_acts2 length: {len(subset_acts2)}")
        return RLPerturbationAgent(self.env.observation_space, self.agent.agent, 0.5, attr_list, attr_start_idx, subset=best_combinations, subset_acts=subset_acts2)

